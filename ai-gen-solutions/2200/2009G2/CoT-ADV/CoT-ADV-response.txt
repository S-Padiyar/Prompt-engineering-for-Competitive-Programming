**Analysis**

We have an array 
```
a[1…n]
``` 
and a fixed window‐length 
```
k
```
.  For any subarray 
```
b = a[l…j]
``` 
with length ≥ k (i.e. j ≥ l+k−1), we define  
```
f(b) 
```
to be the minimum number of point‐updates needed so that *some* length‐k consecutive subarray of `b` becomes a “consecutive sequence” (difference = +1 everywhere).  

A standard re‐interpretation shows that if you look at any window  
```
W = b[i…i+k−1]
``` 
then to turn it into a strictly consecutive run you must modify exactly those positions `t` in the window for which  
```
b[t]−t  ≠  constant.  
``` 
Hence the cost to fix window `W` is  
```
k − (maximum frequency of the value b[t]−t in that window).  
``` 
Therefore  
```
f(b)  =  min_{windows W of length k ⊆ b}  [ k − maxFreq( b[t]−t in W ) ].
```

We are given *q* queries of the form  
```
(l, r)  with  r ≥ l+k−1,
``` 
and we must compute  
\[
  \sum_{j = l+k-1}^{r}  f\bigl(a[l..j]\bigr).
\]

Let us define:

1) Precompute  
   ```
   c[i] = a[i] − i
   ```  
   for `i = 1..n`.

2) Let `M = n−k+1`.  For each window start `i=1..M` we compute  
   ```
   cost[i] = k − (the max frequency of c[ i..i+k−1 ]).
   ```  
   Then for a subarray `b = a[l..j]` with `j≥l+k−1`,  
   ```
   f(a[l..j]) =  min_{i from l..j−k+1} cost[i].
   ```
   In particular, if we set  
   ```
   P[u] = min_{i=l..u} cost[i],     where   u = j−k+1,
   ```  
   then the query sum becomes  
   ```
   Σ_{j=l+k−1..r} f(a[l..j])  
     =  Σ_{u = l..(r−k+1)}  min_{i=l..u} cost[i]
     =  Σ_{u=l..R}  P[u],    where R = r−k+1.
   ```
   Thus each query boils down to: **given the array**  
   ```
   cost[1..M],
   ```  
   answer many queries of the form  
   ```
   (l, R):   sum_{u=l..R}  (   min_{i=l..u} cost[i]   ).
   ```

A well‐known trick to handle “sum of prefix‐minimums” over arbitrary sub‐ranges is:

–  Build the array  
   ```
   cost[1..M]
   ```
   and find, for each index `i`, the nearest strictly smaller element to the left and right:

   • `L[i]` = the last index `< i` with `cost[L[i]]<cost[i]`, or `0` if none.  
   • `R[i]` = the first index `> i` with `cost[R[i]]<cost[i]`, or `M+1` if none.  

  Then on **any** query `(l,R)` the contribution of position `i` to `Σ P[u]` turns out to be
  \[
     \text{(cost[i])} \;\times\; 
        \bigl|\{\,u\in[l..R]:\,i\text{ is the prefix‐min position for }u\}\bigr|
     \;=\;
     cost[i]\;\times\;\max\bigl(0,\min(R,R[i]-1)-\max(l,i)+1\bigr).
  \]
  One checks that `cost[i]` is indeed the prefix minimum from `u=i` up to `u=R[i]-1`, and that it carries over to each `u` in that intersection with `[l..R]` provided `l> L[i]`.

Thus in principle each query is a **sum of weighted interval‐intersections**.  We can algebraically split that intersection‐length into two parts (“does it end before `R` or straddle `R`?”) and reduce to a collection of sums of the form

-  Σ cost[i]·(R[i]−i+1)  over indices i in `[l..R]` with `R[i] ≤ R` and `L[i]<l`,  
-  Σ cost[i]·(R−i+1)      over indices i in `[l..R]` with `R[i] > R` and `L[i]<l`.

Gathering those leads to two types of partial sums:

1.  **Full‐interval contributions**  
    `Σ cost[i]·(R[i]−i+1)` for those with `R[i] ≤ R`.  
2.  **Truncated contributions**  
    `Σ cost[i]` and `Σ cost[i]·i` for those with `R[i] > R`.  

Even so, we still must enforce  
```
i>=l  and  L[i]<l  
```
i.e.  the “activation” condition that the element `i` is a new prefix‐minimum *after* we start our range at `l`.  

Altogether, each query `(l,R)` becomes a **3‐D orthogonal sum** over our points `i=1..M`, with 3 parameters:

-   `t = L[i]+1`  is the “earliest starting prefix” time;
-   `e = R[i]`    is the “upper‐end threshold”;
-   `x = i`       is the actual position.

We must sum weights  
```
w1 = cost[i]*(R[i]-i+1),
w2 = cost[i],
w3 = cost[i]*i,
```
over those points `(t,e,x)` such that

```
t <= l_query   ( i.e. L[i]<l ),
e <= R_query,
l_query <= x <= R_query.
```

We solve this classic 3‐D offline orthogonal‐range‐sum problem by two steps:

1) **Compute the “total active sums”**  
   \( \sum cost[i] \) and \( \sum cost[i]\cdot i\) over all `i` with `L[i]+1 ≤ l`.  We do a simple 1D sweep in order of increasing `l`; we maintain a Fenwick‐tree (BIT) on the `i`‐coordinate to accumulate those two weights as soon as `l` passes `L[i]+1`.  This gives us, for each query, in O(log M) time,  
   ```
   totA[query],  totAi[query]
   = Σ_{i in [l..R] with L[i]+1 ≤ l}  cost[i],   cost[i]*i
   ```

2) **Compute the sums over those with also `e <= R_query`.**  
   We now have *points* `(t,e,i)` with weights `(w1,w2,w3)`, and *queries* `(l_query,R_query,[l_query..R_query])`.  We must add up all points with `t ≤ l_query` and `e ≤ R_query` and `i in [l_query..R_query]`.  That is a standard 3‐D prefix‐sum arrangement which is elegantly done by a **CDQ‐divide‐and‐conquer** on the `t`‐dimension plus a Fenwick‐tree on the `i`‐dimension.  

Overall time is about  
```
O(n + (#windows)) + O((n+q) log n)   // for the sliding‐window + BIT sweep
 +   O((n+q)·log^2 n)                // for the CDQ + Fenwicks
```
which fits under 3 seconds for n,q≤2·10^5.

---

```java
import java.io.*;
import java.util.*;

public class Main {
    static class Node {
        int t, e, i, L, R, id;
        long w1, w2, w3;
        boolean isQuery;
    }

    static int M;           // number of windows = n-k+1
    static int[] prevSm, nextSm;   // prev smaller, next smaller indices
    static int[] cost;      // cost[1..M]
    static long[] totA, totAi;     // totals from the 1D sweep
    static long[] ansC1, ansA, ansAi;  // partial sums from CDQ
    static Node[] events;   // all point+query events
    static long[] BIT1, BIT2, BIT3;  // Fenwick arrays (for 3 weights)
    static int BITn;

    // Fenwick utility: point-update, range-prefix-sum
    static void bitUpdate(int idx, long d1, long d2, long d3) {
        for (int x = idx; x <= BITn; x += x & -x) {
            BIT1[x] += d1;
            BIT2[x] += d2;
            BIT3[x] += d3;
        }
    }
    static long[] bitQuery(int idx) {
        long s1=0, s2=0, s3=0;
        for (int x = idx; x > 0; x -= x & -x) {
            s1 += BIT1[x];
            s2 += BIT2[x];
            s3 += BIT3[x];
        }
        return new long[]{s1, s2, s3};
    }

    // CDQ over events[t=0..N-1], dividing by t
    static void cdq(int L, int R) {
        if (L >= R) return;
        int mid = (L + R) >>> 1;
        cdq(L, mid);
        // collect "points" from [L..mid] and "queries" from [mid+1..R]
        List<Node> pts = new ArrayList<>(), qry = new ArrayList<>();
        for (int i = L; i <= mid; i++) {
            if (!events[i].isQuery) pts.add(events[i]);
        }
        for (int i = mid+1; i <= R; i++) {
            if (events[i].isQuery) qry.add(events[i]);
        }
        // sort by e
        pts.sort(Comparator.comparingInt(a->a.e));
        qry.sort(Comparator.comparingInt(a->a.e));

        // sweep e, maintain Fenwick on the i-coordinate
        int p = 0;
        for (Node Q : qry) {
            while (p < pts.size() && pts.get(p).e <= Q.e) {
                Node P = pts.get(p++);
                // add its weights
                bitUpdate(P.i, P.w1, P.w2, P.w3);
            }
            // now query range [Q.L..Q.R]
            long[] sumRight = bitQuery(Q.R);
            long[] sumLeft  = bitQuery(Q.L - 1);
            ansC1[Q.id] += (sumRight[0] - sumLeft[0]);
            ansA[Q.id]  += (sumRight[1] - sumLeft[1]);
            ansAi[Q.id] += (sumRight[2] - sumLeft[2]);
        }
        // rollback
        for (int i = 0; i < p; i++) {
            Node P = pts.get(i);
            bitUpdate(P.i, -P.w1, -P.w2, -P.w3);
        }

        cdq(mid+1, R);
    }

    public static void main(String[] args) throws IOException {
        BufferedReader br = new BufferedReader(new InputStreamReader(System.in));
        PrintWriter   pw = new PrintWriter(new OutputStreamWriter(System.out));
        StringTokenizer st;

        int T = Integer.parseInt(br.readLine());
        while (T-- > 0) {
            st = new StringTokenizer(br.readLine());
            int n = Integer.parseInt(st.nextToken());
            int k = Integer.parseInt(st.nextToken());
            int q = Integer.parseInt(st.nextToken());
            int[] a = new int[n+1];
            st = new StringTokenizer(br.readLine());
            for (int i = 1; i <= n; i++) a[i] = Integer.parseInt(st.nextToken());

            // If k>n, no valid window => but problem guarantees r>=l+k-1, so this never happens.
            M = n - k + 1;

            // 1) Compute c[i] = a[i]-i, i=1..n
            int[] c = new int[n+1];
            for (int i = 1; i <= n; i++) c[i] = a[i] - i;

            // 2) Slide a window of length k over c[.] to compute cost[i] = k - maxFreq
            cost = new int[M+1];
            HashMap<Integer,int[]> freq = new HashMap<>();
            // freq maps (value c) -> [current count of c in window, freqCount array index]
            // We'll track freqCount[f] = how many distinct c's occur exactly f times
            int[] freqCount = new int[k+1];
            int currMax = 0;
            // init first window i=1..k
            for (int i = 1; i <= k; i++) {
                int v = c[i];
                int[] arr = freq.computeIfAbsent(v, x -> new int[]{0,0});
                int old = arr[0];
                if (old > 0) freqCount[old]--;
                arr[0] = old+1;
                freqCount[old+1]++;
                currMax = Math.max(currMax, old+1);
            }
            cost[1] = k - currMax;

            for (int start = 2; start <= M; start++) {
                // remove c[start-1], add c[start+k-1]
                int drop = c[start-1], add = c[start+k-1];
                {
                    int[] arr = freq.get(drop);
                    int old = arr[0];
                    freqCount[old]--;
                    arr[0] = old-1;
                    if (old-1 > 0) freqCount[old-1]++;
                    if (freqCount[currMax] == 0) currMax--;
                }
                {
                    int[] arr = freq.computeIfAbsent(add, x -> new int[]{0,0});
                    int old = arr[0];
                    if (old > 0) freqCount[old]--;
                    arr[0] = old+1;
                    freqCount[old+1]++;
                    currMax = Math.max(currMax, old+1);
                }
                cost[start] = k - currMax;
            }

            // 3) prev/next smaller for cost[1..M]
            prevSm = new int[M+1];
            nextSm = new int[M+1];
            Deque<Integer> stck = new ArrayDeque<>();
            // previous smaller
            for (int i = 1; i <= M; i++) {
                while (!stck.isEmpty() && cost[stck.peek()] >= cost[i]) stck.pop();
                prevSm[i] = stck.isEmpty() ? 0 : stck.peek();
                stck.push(i);
            }
            stck.clear();
            // next smaller
            for (int i = M; i >= 1; i--) {
                while (!stck.isEmpty() && cost[stck.peek()] > cost[i]) stck.pop();
                nextSm[i] = stck.isEmpty() ? (M+1) : stck.peek();
                stck.push(i);
            }

            // Pre‐compute activation times and end‐times
            // activation t[i] = prevSm[i]+1, end‐time e[i] = nextSm[i]-1
            int[] tact = new int[M+1], eend = new int[M+1];
            for (int i = 1; i <= M; i++) {
                tact[i] = prevSm[i] + 1;
                eend[i] = nextSm[i] - 1;
            }

            // Read queries, convert r->R=r-k+1
            int[] Lq = new int[q], Rq = new int[q];
            List<Integer>[] byL = new ArrayList[M+2];
            for (int i = 1; i <= M; i++) byL[i] = new ArrayList<>();
            for (int i = 0; i < q; i++) {
                st = new StringTokenizer(br.readLine());
                int L = Integer.parseInt(st.nextToken());
                int R = Integer.parseInt(st.nextToken());
                Lq[i] = L;
                Rq[i] = R - k + 1;
                byL[L].add(i);
            }

            // 4) Sweep in order of l to build totA, totAi
            totA  = new long[q];
            totAi = new long[q];
            // Fenwicks for these
            long[] fA   = new long[M+2];
            long[] fAi  = new long[M+2];
            // activation buckets
            List<Integer>[] acts = new ArrayList[M+2];
            for (int i = 1; i <= M; i++) acts[i] = new ArrayList<>();
            for (int i = 1; i <= M; i++) acts[tact[i]].add(i);

            // Fenwick routines for this sweep
            class Fwt {
                void upd(long[] F, int x, long v) {
                    for (; x <= M; x += x&-x) F[x] += v;
                }
                long sum(long[] F, int x) {
                    long s=0;
                    for (; x>0; x-=x&-x) s+=F[x];
                    return s;
                }
                long range(long[] F, int l, int r) {
                    if (l>r) return 0L;
                    return sum(F,r) - sum(F,l-1);
                }
            }
            Fwt fwt = new Fwt();

            for (int l = 1; l <= M; l++) {
                // activate points i with tact[i]==l
                for (int i : acts[l]) {
                    fwt.upd(fA,  i, cost[i]);
                    fwt.upd(fAi, i, (long)cost[i]*i);
                }
                // answer queries that begin at l
                for (int qi : byL[l]) {
                    int Rpos = Rq[qi];
                    totA[qi]  = fwt.range(fA,  l, Rpos);
                    totAi[qi] = fwt.range(fAi, l, Rpos);
                }
            }

            // 5) Build the combined events array for CDQ
            events = new Node[M + q];
            int idx = 0;
            for (int i = 1; i <= M; i++) {
                Node nd = new Node();
                nd.isQuery = false;
                nd.t = tact[i];
                nd.e = eend[i];
                nd.i = i;
                // weights w1,w2,w3:
                long cst = cost[i];
                nd.w2 = cst;               // for Σ cost[i]
                nd.w3 = cst * i;           // for Σ cost[i]*i
                nd.w1 = cst * (eend[i] - i + 1);  // for Σ cost[i]*(end-i+1)
                events[idx++] = nd;
            }
            for (int qi = 0; qi < q; qi++) {
                Node nd = new Node();
                nd.isQuery = true;
                nd.t = Lq[qi];
                nd.e = Rq[qi];
                nd.L = Lq[qi];
                nd.R = Rq[qi];
                nd.id = qi;
                events[idx++] = nd;
            }
            // sort by t, and points before queries at equal t
            Arrays.sort(events, 0, idx,
                (x,y)-> {
                    if (x.t != y.t) return x.t - y.t;
                    return (x.isQuery?1:0) - (y.isQuery?1:0);
                }
            );

            // Prepare arrays for CDQ results
            ansC1 = new long[q];
            ansA  = new long[q];
            ansAi = new long[q];

            // Init Fenwick for CDQ (size M on the i-axis)
            BITn = M;
            BIT1 = new long[M+2];
            BIT2 = new long[M+2];
            BIT3 = new long[M+2];

            // Run CDQ to fill ansC1[], ansA[], ansAi[]
            cdq(0, idx-1);

            // 6) Final assembly of each query's answer
            //    ans = (sum w1 for e<=R) 
            //          + (R+1)*( Σ cost[i] for e>R ) 
            //          - ( Σ cost[i]*i for e>R )
            //    but Σ cost[i] for e>R = totA - ansA[qi], etc.
            StringBuilder sb = new StringBuilder();
            for (int qi = 0; qi < q; qi++) {
                long c1  = ansC1[qi];
                long aLE = ansA[qi];
                long aiLE= ansAi[qi];
                long Aall = totA[qi],  Aiall = totAi[qi];
                long Rv = Rq[qi];
                long truncatedCountA  = Aall   - aLE;
                long truncatedCountAi = Aiall  - aiLE;
                long part2 = (Rv + 1) * truncatedCountA  - truncatedCountAi;
                long answer = c1 + part2;
                sb.append(answer).append('\n');
            }

            pw.print(sb);
            pw.flush();
        }
        pw.close();
    }
}
```

**Explanation of the key steps**

1. **Convert each window of length k into a “cost”**  
   We let  
   \[
     \text{cost}[i]
     = k - \max_{\;i\le t<i+k}\bigl(\text{frequency of }(a[t]-t)\bigr),
   \]
   so that fixing that one window costs exactly that many point‐updates.

2. **Rewriting the query**  
   A query `(l,r)` asks
   \[
     \sum_{j=l+k-1}^r f\bigl(a[l..j]\bigr)
     = \sum_{u=l}^{r-k+1}\,\min_{i=l..u}\,\text{cost}[i].
   \]
   If we denote `M=n-k+1` and let `R=r-k+1`, the sum becomes
   ```
     Σ_{u=l..R} min(cost[l..u]).
   ```

3. **Prefix‐min‐over‐arbitrary‐range**  
   A well‐known data‐structure trick is to note that the *prefix‐minimum* for each `u` can be “charged” to exactly one position `i` (namely the position of the new minima in the running scan).  Precomputing for each `i` the interval `[i..nextSmaller[i]-1]` over which `cost[i]` remains the prefix‐min, we reduce each query to
   ```
     sum_{i} cost[i] × | [i..R_i∧R] ∩ [l..R] |,
   ```
   and that in turn splits into two algebraic pieces.

4. **3D offline sum**  
   We have “points” `i=1..M` each with three weights
   ```
     w1 = cost[i]*(R_i-i+1),
     w2 = cost[i],
     w3 = cost[i]*i
   ```
   at coordinates
   ```
     t = L[i]+1 ,   e = R[i],   x = i
   ```
   and “queries” `(l,R)` that want
   ```
     sum of w1,w2,w3 over all points with t ≤ l and e ≤ R and l ≤ x ≤ R.
   ```
   This is a classic 3D orthogonal‐range sum: we sort everything by the “t” dimension and do a CDQ‐divide‐and‐conquer.  In each CDQ step we merge by the “e” dimension and maintain a Fenwick tree over `x` for the three weights.

5. **Plus the 1D sweep**  
   We also needed, for the “truncated‐end” part, the sum of `w2`=cost[i] and `w3`=cost[i]·i over all `i` with `L[i]+1 ≤ l` and `i∈[l..R]`.  We handle that via a simple one‐dimensional sweep in increasing `l`, also using a BIT on the axis `i`.

Overall the solution runs in roughly  
```
O(n + M)  for the sliding window,
+ O((M+q) log M)  for the 1D sweep,
+ O((M+q) log^2 M)  for the CDQ + Fenwicks,
```
which is fast enough for n,q up to 2·10^5 under a 3 s limit.